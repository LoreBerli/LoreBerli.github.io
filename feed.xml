<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://loreberli.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://loreberli.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-06T09:42:50+00:00</updated><id>https://loreberli.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">AI4Media participation</title><link href="https://loreberli.github.io/blog/2023/ai4media_poster_video/" rel="alternate" type="text/html" title="AI4Media participation"/><published>2023-05-19T09:13:32+00:00</published><updated>2023-05-19T09:13:32+00:00</updated><id>https://loreberli.github.io/blog/2023/ai4media_poster_video</id><content type="html" xml:base="https://loreberli.github.io/blog/2023/ai4media_poster_video/"><![CDATA[<h2 id="ai4media">AI4Media</h2> <p>The 7th AI4Media Plenary Meeting was held at the University of Florence between January 31st and February 1st 2023. During the event, the technical partners of Work Packages 3, 4, 5, and 6 presented their work through posters and demos and demonstrated live the use case demonstrators. A debate space was also provided for the partners to exchange ideas and get to know each other’s work. <a href="https://www.ai4media.eu/">ai4media</a></p> <p>This is my presentation of a work-in-progress paper focused on analog video restoration.</p> <iframe width="560" height="315" src="https://www.youtube.com/embed/FCY9JW-iUN0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>]]></content><author><name></name></author><category term="torch-posts"/><category term="papers,"/><category term="conference,"/><category term="machine"/><category term="learning"/><summary type="html"><![CDATA[AI4Media The 7th AI4Media Plenary Meeting was held at the University of Florence between January 31st and February 1st 2023. During the event, the technical partners of Work Packages 3, 4, 5, and 6 presented their work through posters and demos and demonstrated live the use case demonstrators. A debate space was also provided for the partners to exchange ideas and get to know each other’s work. ai4media]]></summary></entry><entry><title type="html">Autonomous driving research with CARLA simulator</title><link href="https://loreberli.github.io/blog/2022/carlaacmm/" rel="alternate" type="text/html" title="Autonomous driving research with CARLA simulator"/><published>2022-10-10T09:13:32+00:00</published><updated>2022-10-10T09:13:32+00:00</updated><id>https://loreberli.github.io/blog/2022/carlaacmm</id><content type="html" xml:base="https://loreberli.github.io/blog/2022/carlaacmm/"><![CDATA[<p><strong>This is a short version of my article published <a href="https://records.sigmm.org/?open-source-item=autonomous-driving-research-with-carla-simulator">here</a></strong></p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/carla_00-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/carla_00-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/carla_00-1400.webp"/> <img src="/assets/img/carla_00.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><em>Visualization of different data streams generated by the simulator (Depth, RGB, Semantic Segmentation, LiDAR, Optical Flow, Semantic LiDAR).</em></p> <hr/> <h2 id="introduction">Introduction</h2> <p>The autonomous driving industry in order to advance through its six levels of automation (as defined by SAE, Society of Automotive Engineers [1] is going to be increasingly more data-driven. While the amount of sensors and their technology have been increasing it is still both cost effective and in some cases necessary to use a simulator considering that deploying even a single autonomous car could necessitate funds and manpower and still be a liability in terms of safety.</p> <p>A simulator for autonomous driving can provide a safe and virtually cost-free controllable environment for research and development purposes but also for testing dangerous to reproduce corner cases.</p> <p>The CARLA simulator is a free and open source modular framework exposing flexible APIs built on top of the Unreal Engine 4 (UE4) and released under the MIT license. It was developed from the start with the intent of democratizing research in the industry by providing academics and small companies a customizable platform to perform cutting-edge research and development for autonomous driving, bridging the gap with large companies or universities that have access to a large fleet of vehicles or a large collection of data.</p> <p>The simulator was firstly published in a paper <a href="#1">[1]</a> by Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun at the Conference on Robot Learning 2017</p> <p>The CARLA project also includes a series of benchmarks to evaluate the driving ability of autonomous agents in different realistic traffic situations. More info is available at [4] where the CARLA Autonomous Driving Challenge is part of the Machine Learning for Autonomous Driving Workshop at NeurIPS 2021.</p> <h2 id="what-can-carla-be-used-for">What can CARLA be used for</h2> <p>CARLA simulator provides a feature rich framework to test and research a wide range of autonomous driving related tasks. It supplies the user with a digital environment made up of various urban layouts, buildings and vehicles along with a flexible configuration capable of specifying all aspects concerning the simulation. As an example the user has complete control in real time over vehicle and pedestrian traffic and their behavior, traffic lights, weather conditions.</p> <p>Along with the environment also the ego vehicle is highly customizable in terms of its sensor suite. The provided APIs let the user collect data from simulated sensors such as RGB, Depth, Semantic Segmentation cameras and LIDAR. But it is also possible to use less common triggers such as lane-invasion, collision, obstacle and infraction detectors. The APIs grant the user a fine grained control of every detail of the simulation allowing task as data collection for supervised learning, such as shown in <a href="http://dx.doi.org/10.1007/978-3-030-25614-2_10">2</a> where a dataset of semantic segmentation images composed of pairs with and without dynamic actors is collected, or training of a reinforcement learning model, or a imitation learning model as in <a href="http://dx.doi.org/10.1109/icra.2018.8460487">3</a> possible.</p> <h2 id="how-carla-works-under-the-hood">How CARLA works under the hood</h2> <p>By being implemented as an open-source layer over Unreal Engine 4 (UE4) CARLA comes with a state-of-the-art physics and rendering engine. The simulator is implemented as a server-client system in which the server side is in charge of maintaining the state of every actor and the world, physics computation and graphics rendering while the client side is composed by one or multiple clients that connect requesting data and sending commands to control the logic of actors on scene and to set world conditions.</p> <p>To learn more about CARLA we refer readers to its <a href="https://carla.org/">project site</a>.</p> <p>Getting Started Carla offers pre-built releases for both Windows and Linux, but it can be built from source on both systems following the guide <a href="https://arxiv.org/abs/1711.03938">5</a>. Carla is also provided as a Docker container. As of the time of writing CARLA latest version is 0.9.13 and the provided Debian package is available for both Ubuntu 18.04 and Ubuntu 20.04.</p> <p>The recommended requirements suggest a 6GB to 8GB GPU and a dedicated GPU is recommended for training. CARLA uses Python as its main scripting language, supporting Python 2.7 and Python 3 on Linux, and Python 3 on Windows.</p> <p>Installing</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 1AF1527DE64CB8D9
sudo add-apt-repository "deb [arch=amd64] http://dist.carla.org/carla $(lsb_release -sc) main"
</code></pre></div></div> <p>Then</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo apt-get update # Update the Debian package index
sudo apt-get install carla-simulator # Install the latest CARLA version, or update the current installation
cd /opt/carla-simulator # Open the folder where CARLA is installed
</code></pre></div></div> <p>Once installed the server as of 0.9.12 you also need the CARLA client library</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install carla
</code></pre></div></div> <p>And the Pygame library</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install --user pygame numpy
</code></pre></div></div> <h1 id="the-server">The server</h1> <p>To launch the server:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd path/to/carla/root
/CarlaUE4.sh
</code></pre></div></div> <p>Now we should see a pop up window showing the fully navigable environment in the spectator view, the server is now running and waiting for a client to connect, by default on port 2000. o launch the server headless, or off-screen:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./CarlaUE4.sh -RenderOffScreen
</code></pre></div></div> <p>Next step is to develop a client script that will interact with the Actors inside the CARLA environments. The repo provides multiple base clients examples here.</p> <h1 id="the-client">The client</h1> <p>We’ll go over some of the introductory examples to show some of the basic concepts.</p> <p>We start by creating a client and connecting to our carla server running on port 2000</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>client = carla.Client('localhost', 2000)
client.set_timeout(2.0)
</code></pre></div></div> <p>Then we get the current environment state:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>world = client.get_world()
</code></pre></div></div> <p>In order to access, customize and finally instantiate actors in the scene we need to familiarize with blueprints first. Blueprints are premade layouts with animations and a series of attributes such as vehicle color, amount of channels in a lidar sensor, a walker’s speed, and much more.</p> <p>Once we accessed the library:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>blueprint_library = world.get_blueprint_library()
</code></pre></div></div> <p>We can filter its contents by using wildcard patterns, instantiate it and spawn it in the world:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vehicle_bp = random.choice(blueprint_library.filter('vehicle.*.*'))
transform = random.choice(world.get_map().get_spawn_points())
vehicle = world.spawn_actor(vehicle_bp, transform)
</code></pre></div></div> <p>Now we can get to the data collections tools, and mount some sensors on our vehicle:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>camera_bp = blueprint_library.find('sensor.camera.depth')
camera_transform = carla.Transform(carla.Location(x=1.5, z=2.4))
camera = world.spawn_actor(camera_bp, camera_transform, attach_to=vehicle)
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/carla_01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/carla_01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/carla_01-1400.webp"/> <img src="/assets/img/carla_01.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="references">References</h2> <p><a id="1" href="https://www.sae.org/standards/content/j3016_201806/">[1] CARLA</a></p>]]></content><author><name></name></author><category term="software"/><category term="research"/><category term="synthetic-data"/><category term="software,"/><category term="python,"/><category term="machine"/><category term="learning"/><summary type="html"><![CDATA[Brief tutorial for CARLA simulator]]></summary></entry><entry><title type="html">Identity Crisis: Memorization and Generalization under Extreme Overparameterization - ICRL2020</title><link href="https://loreberli.github.io/blog/2021/p000-identity-crisis/" rel="alternate" type="text/html" title="Identity Crisis: Memorization and Generalization under Extreme Overparameterization - ICRL2020"/><published>2021-02-04T09:13:32+00:00</published><updated>2021-02-04T09:13:32+00:00</updated><id>https://loreberli.github.io/blog/2021/p000-identity-crisis</id><content type="html" xml:base="https://loreberli.github.io/blog/2021/p000-identity-crisis/"><![CDATA[<h2 id="what-is-inductive-or-learning-bias">What is <em>inductive</em> or <em>learning</em> bias?</h2> <p>In a machine learning model <em>inductive bias</em> refers to the set of assumptions affecting</p> <h2 id="how-to-study-it">How to study it?</h2> <p>In this paper a framework to better analyze inductive bias is presented. Starting from the observation that <em>modern deep NNs</em> are largely <strong>overparameterized</strong>.</p> <p>Predictions in the vicinity of the training samples are regularized by continuity or smoothness constraints, while predictions further away from training samples determine <strong>generalizations</strong> performances. The different behaviors reflect the learning bias of the model and the learning algorithm.</p> <div class="row"> <div class="col-4"><figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/identity-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/identity-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/identity-1400.webp"/> <img src="/assets/img/identity.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col">Predictions in the vicinity of the training samples are regularized by continuity or smoothness constraints, while predictions further away from training samples determine **generalizations** performances. The different behaviors reflect the learning bias of the model and the learning algorithm.</div> </div> <p>In order to better understand this behavior the authors use two approaches:</p> <ul> <li>Learning with one example <ul> <li>Extreme overparameterization</li> <li>Easy to determine distance from the training example</li> </ul> </li> <li>Learning the identity map \(f(x)=x\) <ul> <li>Requires all input features to be transmitted to the output</li> <li>Affords detailed analysis of visualization and model behaviour</li> </ul> </li> </ul> <hr/> <p>Find the paper here! <a href="https://iclr.cc/virtual_2020/poster_B1l6y0VFPr.html">ICLR2020</a>.</p>]]></content><author><name></name></author><category term="papers"/><category term="research"/><summary type="html"><![CDATA[What is inductive or learning bias? In a machine learning model inductive bias refers to the set of assumptions affecting How to study it? In this paper a framework to better analyze inductive bias is presented. Starting from the observation that modern deep NNs are largely overparameterized.]]></summary></entry></feed>